{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "basedir = \"/media/pathologyhd/path_nlp/pathparsing/\"\n",
    "sys.path.append(\"/media/pathologyhd/path_nlp/pathparsing/Prostate-Pathology-Parser\")\n",
    "\n",
    "from methods.bag_of_ngrams.processing import cleanSplit, getTrainedVectorizer, STRIPCHARS\n",
    "from methods.extraction.general import getCounter, sampleTrain\n",
    "from methods.extraction.token import getX, getY\n",
    "from methods.sklearn_calibration import *\n",
    "from random import sample\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from pyfunctions.general import *\n",
    "from pyfunctions.pathology import getProstateStageInverseMapping, label_correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set up: data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set arguments\n",
    "args = {'domain': 'prostate',\n",
    "        'path': basedir + \"prostate-open-source/\",\n",
    "        'target_fields': ['ProstateWeight', 'TumorVolume', 'TStage', 'NStage', 'MStage'],\n",
    "        'sample': 20000, # Number of positive and negative tokens to sample for training\n",
    "        'N': 3, # N in N-grams\n",
    "        'k': 5 # Size of context\n",
    "        }\n",
    "\n",
    "# Read in data\n",
    "path = args['path'] + \"data/splits/\" + args['domain'] + \".json\"\n",
    "data = readJson(path)\n",
    "\n",
    "# Process reports\n",
    "data = cleanSplit(data, STRIPCHARS)\n",
    "\n",
    "# Get counters and vectorizers\n",
    "trainReports = extractListFromDic(data['train'], 'clean_document')\n",
    "\n",
    "# Maps text to vectors based on counts of words\n",
    "args['vectorizer'] = getTrainedVectorizer(trainReports, args['N'], 1)\n",
    "args['counter'] = getCounter(trainReports)\n",
    "args['stage_mapping'] = getProstateStageInverseMapping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Save document matrices and label arrays\n",
    "\n",
    "- Represent each document as a matrix where rows represents each word in the report and the columns represent the features of each word (context words and word type)\n",
    "- Label is 1 if word matches ground-truth and 0 if not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "y_train = {field:[] for field in args['target_fields']}\n",
    "\n",
    "for i, patient in enumerate(data['train']):\n",
    "    try:\n",
    "        X = getX(patient['clean_document'], args)\n",
    "        X_train.append(csr_matrix(X))\n",
    "\n",
    "        for field in args['target_fields']:\n",
    "            stage = 'stage' in field.lower()\n",
    "            ys = np.zeros(len(patient['clean_document'].split()))\n",
    "            keyLabels =  extractListFromDic(data['train'], 'labels', field)\n",
    "            y = getY(patient['clean_document'], keyLabels[i], args, stage)\n",
    "            y_train[field] = y_train[field] + y.tolist()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Validation and test data\n",
    "- Calculate these matrices for validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "X_val, X_test = [], []\n",
    "\n",
    "for i, patient in enumerate(data['val']):\n",
    "    try:\n",
    "        X_val.append(getX(patient['clean_document'], args))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "for i, patient in enumerate(data['test']):\n",
    "    try:\n",
    "        X_test.append(getX(patient['clean_document'], args))\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train token extraction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators='warn', n_jobs=None, oob_score=False,\n",
       "            random_state=None, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=40, n_jobs=40,\n",
       "          param_distributions={'bootstrap': [True, False], 'max_depth': [10, 20, 30, 40, 50, None], 'max_features': ['auto', 'sqrt'], 'min_samples_leaf': [1, 2, 4, 8, 16, 32, 64], 'n_estimators': [200, 400, 600, 800, 1000]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "* Set parameters and models\n",
    "\"\"\"\n",
    "params = {'bootstrap': [True, False],\n",
    "          'max_depth': [10, 20, 30, 40, 50, None],\n",
    "          'max_features': ['auto', 'sqrt'],\n",
    "          'min_samples_leaf': [1, 2, 4, 8, 16, 32, 64],\n",
    "          'n_estimators': [200, 400, 600, 800, 1000]}\n",
    "\n",
    "args['key'] = 'TumorVolume'\n",
    "\n",
    "\"\"\"\n",
    "* Run model and return probabilities of tokens\n",
    "\"\"\"\n",
    "X_train_sampled, y_train_sampled = sampleTrain(vstack(X_train), np.array(y_train[args['key']]), args)\n",
    "\n",
    "clf = RandomForestClassifier(class_weight = 'balanced')\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=params, n_iter=40, cv=3, n_jobs=40)\n",
    "random_search.fit(X_train_sampled, y_train_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Extract predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = pd.DataFrame(columns = ['label', 'predicted_token', 'y_prob', 'word_ind'])\n",
    "\n",
    "for i, patient in enumerate(data['val']):    \n",
    "    y_proba = random_search.predict_proba(X_val[i])\n",
    "    y_proba = [p[1] for p in y_proba]\n",
    "    \n",
    "    inds = np.argsort(y_proba)[::-1]\n",
    "    best = patient['clean_document'].split()[inds[0]]\n",
    "    word_ind = inds[0]\n",
    "    val_predictions.loc[i] = [patient['labels'][args['key']], best, y_proba[inds[0]], word_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = pd.DataFrame(columns = ['label', 'predicted_token', 'y_prob', 'word_ind'])\n",
    "\n",
    "for i, patient in enumerate(data['test']):    \n",
    "    y_proba = random_search.predict_proba(X_test[i])\n",
    "    y_proba = [p[1] for p in y_proba]\n",
    "    \n",
    "    inds = np.argsort(y_proba)[::-1]\n",
    "    best = patient['clean_document'].split()[inds[0]]\n",
    "    word_ind = inds[0]\n",
    "    test_predictions.loc[i] = [patient['labels'][args['key']], best, y_proba[inds[0]], word_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Label correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = label_correctness(val_predictions, args['key'])\n",
    "test_predictions = label_correctness(test_predictions, args['key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Calibration and expected calibration error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = val_predictions['y_prob']\n",
    "y_val = val_predictions['correct']\n",
    "X_val = np.array(X_val).reshape(len(y_val))\n",
    "\n",
    "if len(np.unique(y_val)) > 1:\n",
    "    reg = IsotonicRegression()\n",
    "    reg.fit(X_val, y_val)\n",
    "\n",
    "    X_test = test_predictions['y_prob']\n",
    "    y_test = test_predictions['correct']\n",
    "    X_test = np.array(X_test).reshape(len(y_test))\n",
    "\n",
    "    X_test[X_test < reg.X_min_] = reg.X_min_\n",
    "    X_test[X_test > reg.X_max_] = reg.X_max_\n",
    "\n",
    "    test_predictions['calibrated_score'] = reg.predict(X_test)\n",
    "\n",
    "    ece_error = ece_mce_error(reg.predict(X_test), test_predictions['final_prediction'], test_predictions['label'],\n",
    "                              num_bins=10, plot=None)\n",
    "    \n",
    "    print('expected calibration error:', ece[0])\n",
    "else:\n",
    "    # Nothing to do here (cannot calibrate with only 1 class)\n",
    "    test_predictions['calibrated_score'] = test_predictions['y_prob']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
