{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.insert(0,'../..')\n",
    "\n",
    "from methods.bag_of_ngrams.processing import cleanSplit, getTrainedVectorizer, STRIPCHARS\n",
    "from methods.extraction.general import getCounter, sampleTrain\n",
    "from methods.extraction.token import getX, getY\n",
    "from methods.sklearn_calibration import *\n",
    "from random import sample\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from pyfunctions.general import *\n",
    "from pyfunctions.pathology import getProstateStageInverseMapping, label_correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set up: data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set arguments\n",
    "args = {'domain': 'prostate',\n",
    "        'target_fields': ['ProstateWeight', 'TumorVolume', 'TStage', 'NStage', 'MStage'],\n",
    "        'sample': 20000, # Number of positive and negative tokens to sample for training\n",
    "        'N': 3, # N in N-grams\n",
    "        'k': 5 # Size of context\n",
    "        }\n",
    "\n",
    "# Read in data\n",
    "path = \"../../data/\" + args['domain'] + \".json\"\n",
    "data = readJson(path)\n",
    "\n",
    "# Process reports\n",
    "data = cleanSplit(data, STRIPCHARS)\n",
    "\n",
    "# Get counters and vectorizers\n",
    "trainReports = extractListFromDic(data['train'], 'clean_document')\n",
    "\n",
    "# Maps text to vectors based on counts of words\n",
    "args['vectorizer'] = getTrainedVectorizer(trainReports, args['N'], 1)\n",
    "args['counter'] = getCounter(trainReports)\n",
    "args['stage_mapping'] = getProstateStageInverseMapping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Save document matrices and label arrays\n",
    "\n",
    "- Represent each document as a matrix where rows represents each word in the report and the columns represent the features of each word (context words and word type)\n",
    "- Label is 1 if word matches ground-truth and 0 if not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = {field:[] for field in args['target_fields']}\n",
    "\n",
    "for i, patient in enumerate(data['train']):\n",
    "    try:\n",
    "        X = getX(patient['clean_document'], args)\n",
    "        X_train.append(csr_matrix(X))\n",
    "\n",
    "        for field in args['target_fields']:\n",
    "            stage = 'stage' in field.lower()\n",
    "            ys = np.zeros(len(patient['clean_document'].split()))\n",
    "            keyLabels =  extractListFromDic(data['train'], 'labels', field)\n",
    "            y = getY(patient['clean_document'], keyLabels[i], args, stage)\n",
    "            y_train[field] = y_train[field] + y.tolist()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Validation and test data\n",
    "- Calculate these matrices for validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_test = [], []\n",
    "val_set, test_set = [], []\n",
    "\n",
    "for i, patient in enumerate(data['val']):\n",
    "    try:\n",
    "        X_val.append(getX(patient['clean_document'], args))\n",
    "        val_set.append(patient)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "for i, patient in enumerate(data['test']):\n",
    "    try:\n",
    "        X_test.append(getX(patient['clean_document'], args))\n",
    "        test_set.append(patient)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train token extraction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* Set parameters and models\n",
    "\"\"\"\n",
    "params = {'bootstrap': [True, False],\n",
    "          'max_depth': [10, 20, 30, 40, 50, None],\n",
    "          'max_features': ['auto', 'sqrt'],\n",
    "          'min_samples_leaf': [1, 2, 4, 8, 16, 32, 64],\n",
    "          'n_estimators': [200, 400, 600, 800, 1000]}\n",
    "\n",
    "args['key'] = 'ProstateWeight'\n",
    "\n",
    "\"\"\"\n",
    "* Run model and return probabilities of tokens\n",
    "\"\"\"\n",
    "X_train_sampled, y_train_sampled = sampleTrain(vstack(X_train), np.array(y_train[args['key']]), args)\n",
    "\n",
    "clf = RandomForestClassifier(class_weight = 'balanced')\n",
    "#random_search = RandomizedSearchCV(clf, param_distributions=params, n_iter=40, cv=3, n_jobs=40)\n",
    "random_search = clf\n",
    "random_search.fit(X_train_sampled, y_train_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Extract predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = pd.DataFrame(columns = ['label', 'predicted_token', 'y_prob', 'word_ind'])\n",
    "\n",
    "for i, patient in enumerate(val_set):\n",
    "    y_proba = random_search.predict_proba(X_val[i])\n",
    "    y_proba = [p[1] for p in y_proba]\n",
    "    \n",
    "    inds = np.argsort(y_proba)[::-1]\n",
    "    best = patient['clean_document'].split()[inds[0]]\n",
    "    word_ind = inds[0]\n",
    "    val_predictions.loc[i] = [patient['labels'][args['key']], best, y_proba[inds[0]], word_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = pd.DataFrame(columns = ['label', 'predicted_token', 'y_prob', 'word_ind'])\n",
    "\n",
    "for i, patient in enumerate(test_set):    \n",
    "    y_proba = random_search.predict_proba(X_test[i])\n",
    "    y_proba = [p[1] for p in y_proba]\n",
    "    \n",
    "    inds = np.argsort(y_proba)[::-1]\n",
    "    best = patient['clean_document'].split()[inds[0]]\n",
    "    word_ind = inds[0]\n",
    "    test_predictions.loc[i] = [patient['labels'][args['key']], best, y_proba[inds[0]], word_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Label correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = label_correctness(val_predictions, args['key'])\n",
    "test_predictions = label_correctness(test_predictions, args['key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Calibration and expected calibration error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_cal = val_predictions['y_prob']\n",
    "y_val_cal = val_predictions['correct']\n",
    "X_val_cal = np.array(X_val_cal).reshape(len(y_val_cal))\n",
    "\n",
    "if len(np.unique(y_val_cal)) > 1:\n",
    "    reg = IsotonicRegression()\n",
    "    reg.fit(X_val_cal, y_val_cal)\n",
    "\n",
    "    X_test_cal = test_predictions['y_prob']\n",
    "    y_test_cal = test_predictions['correct']\n",
    "    X_test_cal = np.array(X_test_cal).reshape(len(y_test_cal))\n",
    "\n",
    "    X_test_cal[X_test_cal < reg.X_min_] = reg.X_min_\n",
    "    X_test_cal[X_test_cal > reg.X_max_] = reg.X_max_\n",
    "\n",
    "    test_predictions['calibrated_score'] = reg.predict(X_test_cal)\n",
    "\n",
    "    ece_error = ece_mce_error(reg.predict(X_test_cal), test_predictions['final_prediction'], test_predictions['label'],\n",
    "                              num_bins=10, plot=None)\n",
    "    \n",
    "    print('expected calibration error:', ece_error[0])\n",
    "else:\n",
    "    # Nothing to do here (cannot calibrate with only 1 class)\n",
    "    test_predictions['calibrated_score'] = test_predictions['y_prob']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
