{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "basedir = \"/media/pathologyhd/path_nlp/pathparsing/\"\n",
    "sys.path.append(basedir + \"prostate-open-source/\")\n",
    "\n",
    "from methods.bag_of_ngrams.processing import (cleanReport, cleanReports, cleanSplit, getCounter, \n",
    "                                              getTrainedVectorizer, STRIPCHARS, unkReports)\n",
    "from methods.sklearn_calibration import *\n",
    "from methods.torch.evaluation import getPredsLabels, getScores\n",
    "from methods.torch.modeling import runModel\n",
    "from methods.torch.models import CnnClassifier\n",
    "from methods.torch.processing import encodeLabels, getEncoder, getTorchLoader, getVocab, reSample\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from pyfunctions.general import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'domain': 'prostate',\n",
    "        'epochs': 20,\n",
    "        'embeddingDim': 300,\n",
    "        'maxDocLength': 1346,\n",
    "        'path': basedir + \"prostate-open-source/\",\n",
    "        'target_fields': ['TreatmentEffect','TumorType','PrimaryGleason','SecondaryGleason','TertiaryGleason',\n",
    "                          'SeminalVesicleNone','LymphNodesNone','MarginStatusNone','ExtraprostaticExtension',\n",
    "                          'PerineuralInfiltration','RbCribriform','BenignMargins'],\n",
    "        'n_tries': 20 # Number of random search candidates\n",
    "        }\n",
    "\n",
    "# Read in data\n",
    "path = args['path'] + \"data/splits/\" + args['domain'] + \".json\"\n",
    "data = readJson(path)\n",
    "\n",
    "# Process reports\n",
    "data = cleanSplit(data, STRIPCHARS)\n",
    "\n",
    "# Unk rare words\n",
    "counter = getCounter(data['train'])\n",
    "data['train'] = unkReports(data['train'], counter)\n",
    "data['val'] = unkReports(data['val'], counter)\n",
    "data['test'] = unkReports(data['test'], counter)\n",
    "\n",
    "# Get vocab\n",
    "vocab = getVocab(data['train'])\n",
    "args['word2idx'] = {word: i for i, word in enumerate(vocab)}\n",
    "args['word2idx']['<unk>']= len(vocab)\n",
    "args['wordDim'] = len(vocab) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Random search parameters\n",
    "params = { 'lr': np.logspace(-6,-1,1000),\n",
    "        'filterNum': [50, 100, 150, 200, 250, 300, 400],\n",
    "        'dropOut': [0, 0.125, 0.25, 0.5],\n",
    "       'filters': [[3], [3,4], [4], [4,5], [5], [5,6], [6]],\n",
    "        'filter_ind': [0,1,2,3,4,5,6],\n",
    "        'batch_size' : [16, 32],  \n",
    "        'epochs': [50]      \n",
    "       }\n",
    "\n",
    "field = args['target_fields'][2]\n",
    "    \n",
    "# Encode labels into 0, 1, 2 values\n",
    "encoder = getEncoder(data['train'] + data['val'] + data['test'], field)\n",
    "\n",
    "data['train'] = encodeLabels(data['train'], encoder, field)\n",
    "data['val'] = encodeLabels(data['val'], encoder, field)\n",
    "data['test'] = encodeLabels(data['test'], encoder, field)\n",
    "data['dev_test'] = encodeLabels(data['test'], encoder, field)\n",
    "args['classSize'] = len(encoder.classes_)\n",
    "\n",
    "# Extract labels and reports\n",
    "corpus_train = extractListFromDic(data['train'], 'clean_document_unked')\n",
    "labels_train = extractListFromDic(data['train'], 'encoded_labels', field)\n",
    "\n",
    "corpus_val = extractListFromDic(data['val'], 'clean_document_unked')\n",
    "labels_val = extractListFromDic(data['val'], 'encoded_labels', field)\n",
    "\n",
    "corpus_test = extractListFromDic(data['test'], 'clean_document_unked')\n",
    "labels_test = extractListFromDic(data['test'], 'encoded_labels', field)\n",
    "\n",
    "# Upsample minority classes\n",
    "corpus_train, labels_train = reSample(corpus_train, labels_train) \n",
    "\n",
    "best_args = {'score': 0}\n",
    "\n",
    "# Loop over number of random search tries\n",
    "for i in range(args['n_tries']):\n",
    "    print(i)\n",
    "    # Set random search parameter configuration       \n",
    "    args['lr'] = np.random.choice(params['lr'])\n",
    "    args['dropOut'] = np.random.choice(params['dropOut'])\n",
    "    args['filters'] = params['filters'][np.random.choice(params['filter_ind'])]\n",
    "    args['batchSize'] = int(np.random.choice(params['batch_size']))\n",
    "    args['filterNum'] = [int(np.random.choice(params['filterNum'])/len(args['filters']))]*len(args['filters'])\n",
    "    args['epochs'] = np.random.choice(params['epochs'])\n",
    "\n",
    "    # Initialize torch loaders\n",
    "    trainLoader = getTorchLoader(corpus_train, labels_train, args, shuffle = True)\n",
    "    valLoader = getTorchLoader(corpus_val, labels_val, args, shuffle = False)\n",
    "    testLoader = getTorchLoader(corpus_test, labels_test, args, shuffle = False)\n",
    "\n",
    "    # Train model\n",
    "    model = CnnClassifier(args)\n",
    "    model = runModel(model, trainLoader, valLoader, args) \n",
    "    val_scores = getScores(model, valLoader, cuda=True)\n",
    "\n",
    "    if val_scores['f1_weighted'] > best_args['score']:\n",
    "        best_args['score'] = val_scores['f1_weighted']\n",
    "        best_args['lr'] = args['lr']\n",
    "        best_args['dropOut'] = args['dropOut']\n",
    "        best_args['filterNum'] = args['filterNum']\n",
    "        best_args['batchSize'] = args['batchSize']\n",
    "        best_args['filters'] = args['filters']\n",
    "        best_args['epochs'] = args['epochs']\n",
    "\n",
    "# Read in best parameters and retrain model\n",
    "args['lr'] = best_args['lr']\n",
    "args['dropOut'] = best_args['dropOut']\n",
    "args['filterNum'] = best_args['filterNum']\n",
    "args['filters'] = best_args['filters'] \n",
    "args['batchSize'] = best_args['batchSize']\n",
    "args['epochs'] = best_args['epochs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with best parameters\n",
    "model = CnnClassifier(args)\n",
    "model = runModel(model, trainLoader, valLoader, args)\n",
    "\n",
    "# Get predictions and labels\n",
    "preds_test, labels_test, probs_test = getPredsLabels(model, testLoader, probs=True, cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'labels': encoder.inverse_transform(labels_test), \n",
    "              'predictions': encoder.inverse_transform(preds_test)}).head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
